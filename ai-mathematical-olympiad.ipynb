{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a807dd",
   "metadata": {
    "papermill": {
     "duration": 0.00604,
     "end_time": "2024-06-26T21:47:02.107914",
     "exception": false,
     "start_time": "2024-06-26T21:47:02.101874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc57b5c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-26T21:47:02.120699Z",
     "iopub.status.busy": "2024-06-26T21:47:02.120041Z",
     "iopub.status.idle": "2024-06-26T21:47:33.007743Z",
     "shell.execute_reply": "2024-06-26T21:47:33.006523Z"
    },
    "papermill": {
     "duration": 30.896692,
     "end_time": "2024-06-26T21:47:33.010210",
     "exception": false,
     "start_time": "2024-06-26T21:47:02.113518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q --no-index /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/bitsandbytes-0.42.0-py3-none-any.whl --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-hf-llms\n",
    "!pip install -q --no-index /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/accelerate-0.27.2-py3-none-any.whl --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-hf-llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6be2e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T21:47:33.024074Z",
     "iopub.status.busy": "2024-06-26T21:47:33.023746Z",
     "iopub.status.idle": "2024-06-26T21:47:40.835266Z",
     "shell.execute_reply": "2024-06-26T21:47:40.834417Z"
    },
    "papermill": {
     "duration": 7.821334,
     "end_time": "2024-06-26T21:47:40.837920",
     "exception": false,
     "start_time": "2024-06-26T21:47:33.016586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import ast\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import subprocess\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, GenerationConfig, BitsAndBytesConfig, PhrasalConstraint\n",
    "\n",
    "#torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "#torch.backends.cuda.enable_flash_sdp(False)\n",
    "#os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "IS_TEST = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5eb7e7",
   "metadata": {
    "papermill": {
     "duration": 0.006106,
     "end_time": "2024-06-26T21:47:40.850781",
     "exception": false,
     "start_time": "2024-06-26T21:47:40.844675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e8d8acc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T21:47:40.864871Z",
     "iopub.status.busy": "2024-06-26T21:47:40.864365Z",
     "iopub.status.idle": "2024-06-26T21:49:23.236967Z",
     "shell.execute_reply": "2024-06-26T21:49:23.235967Z"
    },
    "papermill": {
     "duration": 102.382198,
     "end_time": "2024-06-26T21:49:23.239325",
     "exception": false,
     "start_time": "2024-06-26T21:47:40.857127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581c1b0b3cb5451b9b15e35766f7fa49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"/kaggle/input/deepseek-math\"\n",
    "torch_dtype = torch.float16\n",
    "\n",
    "# Define quantization\n",
    "quantize = False\n",
    "quantize_type = \"8-bit\"\n",
    "quantization_config = None\n",
    "\n",
    "if quantize:\n",
    "    if quantize_type == \"4-bit\":\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,  \n",
    "            bnb_4bit_quant_type=\"nf4\", \n",
    "            bnb_4bit_compute_dtype=torch_dtype,\n",
    "            bnb_4bit_use_double_quant=True)\n",
    "        \n",
    "    if quantize_type == \"8-bit\":\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    \n",
    "# Load model configs\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "# Load model tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",  \n",
    "    torch_dtype=torch_dtype, \n",
    "    config=config,\n",
    "    quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd20f1ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T21:49:23.253351Z",
     "iopub.status.busy": "2024-06-26T21:49:23.252945Z",
     "iopub.status.idle": "2024-06-26T21:49:23.264643Z",
     "shell.execute_reply": "2024-06-26T21:49:23.263949Z"
    },
    "papermill": {
     "duration": 0.02079,
     "end_time": "2024-06-26T21:49:23.266659",
     "exception": false,
     "start_time": "2024-06-26T21:49:23.245869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generation config fo the model\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "\n",
    "# Set some generation configs for the model\n",
    "model.generation_config.do_sample = True\n",
    "model.generation_config.max_new_tokens = 3600\n",
    "model.generation_config.remove_invalid_values = True\n",
    "model.generation_config.top_k = 50\n",
    "model.generation_config.temperature = 1.25\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "model.generation_config.renormalize_logits = True\n",
    "#model.generation_config.penalty_alpha = 0.95\n",
    "\n",
    "# Skip python word\n",
    "bad_words_ids = []\n",
    "bad_words_ids.append(tokenizer(\"python\", add_special_tokens=False).input_ids)\n",
    "bad_words_ids.append(tokenizer(\"Python\", add_special_tokens=False).input_ids)\n",
    "bad_words_ids.append(tokenizer(\"```python\", add_special_tokens=False).input_ids)\n",
    "bad_words_ids.append(tokenizer(\"```Python\", add_special_tokens=False).input_ids)\n",
    "bad_words_ids.append(tokenizer(\"```\", add_special_tokens=False).input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45924089",
   "metadata": {
    "papermill": {
     "duration": 0.005853,
     "end_time": "2024-06-26T21:49:23.278627",
     "exception": false,
     "start_time": "2024-06-26T21:49:23.272774",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668712ef",
   "metadata": {
    "papermill": {
     "duration": 0.005804,
     "end_time": "2024-06-26T21:49:23.290519",
     "exception": false,
     "start_time": "2024-06-26T21:49:23.284715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b60c915",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T21:49:23.303975Z",
     "iopub.status.busy": "2024-06-26T21:49:23.303413Z",
     "iopub.status.idle": "2024-06-26T21:49:23.314809Z",
     "shell.execute_reply": "2024-06-26T21:49:23.313958Z"
    },
    "papermill": {
     "duration": 0.020061,
     "end_time": "2024-06-26T21:49:23.316579",
     "exception": false,
     "start_time": "2024-06-26T21:49:23.296518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def naive_parse(answer):\n",
    "    out = [] \n",
    "    start = False \n",
    "    end = False  \n",
    "    for l in reversed(list(answer)): \n",
    "        if l in '0123456789' and not end:  \n",
    "            start = True  \n",
    "            out.append(l)\n",
    "        else:\n",
    "            if start:  \n",
    "                end = True  \n",
    "        \n",
    "    out = reversed(out)  \n",
    "    return ''.join(out)\n",
    "\n",
    "\n",
    "def process_output(output):\n",
    "    result = output   \n",
    "    flag = True\n",
    "    \n",
    "    try:\n",
    "        result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', result)\n",
    "        \n",
    "        if not len(result_output):\n",
    "            result_output = re.findall(r'The answer is: \\$(\\d+)\\$', result)\n",
    "            \n",
    "            if not len(result_output):\n",
    "                result_output = re.findall(r'The answer is: (\\d+)', result)\n",
    "                \n",
    "                if not len(result_output):\n",
    "                    result_output = re.findall(r'The answer is: \\\\[(\\d+)\\\\]', result)\n",
    "                    \n",
    "                    if not len(result_output):\n",
    "                        result_output = re.findall(r'The answer is \\$(\\d+)\\$', result)\n",
    "                    \n",
    "                        if not len(result_output):\n",
    "                            result_output = naive_parse(result)\n",
    "                            flag = True\n",
    "                            return result_output, flag\n",
    "                        \n",
    "                        else:\n",
    "                            result_output = result_output[-1]\n",
    "                        \n",
    "                    else:\n",
    "                        result_output = result_output[-1]\n",
    "                        \n",
    "                else:\n",
    "                    result_output = result_output[-1] \n",
    "                    \n",
    "            else:\n",
    "                result_output = result_output[-1]\n",
    "                \n",
    "        else:\n",
    "            result_output = result_output[-1]\n",
    "\n",
    "        result_output = int(round(float(eval(result_output))) % 1000)\n",
    "        flag = True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"\\nNot valid result!\")\n",
    "        result_output = random.randint(0, 999)\n",
    "        flag = False\n",
    "        \n",
    "    return result_output, flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0807b9c0",
   "metadata": {
    "papermill": {
     "duration": 0.005696,
     "end_time": "2024-06-26T21:49:23.328106",
     "exception": false,
     "start_time": "2024-06-26T21:49:23.322410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Single and Few Shot generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e5a1e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T21:49:23.340674Z",
     "iopub.status.busy": "2024-06-26T21:49:23.340402Z",
     "iopub.status.idle": "2024-06-26T21:49:23.354966Z",
     "shell.execute_reply": "2024-06-26T21:49:23.354086Z"
    },
    "papermill": {
     "duration": 0.023088,
     "end_time": "2024-06-26T21:49:23.356856",
     "exception": false,
     "start_time": "2024-06-26T21:49:23.333768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generation(messages):    \n",
    "    input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_tensor.to(model.device), bad_words_ids=bad_words_ids)\n",
    "    output = tokenizer.decode(outputs[0][input_tensor.shape[1] : ], skip_special_tokens=True)    \n",
    "    answer, correct = process_output(output)\n",
    "    print(f\"\\nCurrent predicted answer: {answer}. Is correct: {correct}.\\n\\n\\n\")\n",
    "    return answer, correct\n",
    "\n",
    "\n",
    "def few_shot_generation(problem, examples, task_prompt, n_shots=2):\n",
    "    messages = []\n",
    "    \n",
    "    keys = list(examples.keys())    \n",
    "    keys = random.sample(keys, n_shots)\n",
    "    for key in keys:\n",
    "        example_problem = examples[key]\n",
    "        messages.append({\"role\": \"user\", \"content\": example_problem['problem'] + \"\\n\" + task_prompt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": \"\\n\" + example_problem['solution']})\n",
    "        \n",
    "    messages.append({\"role\": \"user\", \"content\": problem + \"\\n\" + task_prompt})\n",
    "    answer, correct = generation(messages)\n",
    "    return answer, correct\n",
    "\n",
    "\n",
    "def single_shot_generation(problem, task_prompt):\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": problem + \"\\n\" + task_prompt})\n",
    "    answer, correct = generation(messages)\n",
    "    return answer, correct\n",
    "   \n",
    "\n",
    "def answer_generation(problem, examples, task_prompts, mode=\"few\", n_shots=2, n_iteration=1, result_strategy=\"most_common\"):\n",
    "    answers = []\n",
    "    for _ in range(n_iteration):\n",
    "        for task_prompt in task_prompts:\n",
    "            try:\n",
    "                if mode == \"single\":\n",
    "                    answer, correct = single_shot_generation(problem, task_prompt)\n",
    "                if mode == \"few\":\n",
    "                    answer, correct = few_shot_generation(problem, examples, task_prompt, n_shots)\n",
    "                if mode == \"tot\":\n",
    "                    answer, correct = single_shot_generation(problem, task_prompt)\n",
    "                    \n",
    "                if correct:\n",
    "                    answers.append(answer)\n",
    "                else: \n",
    "                    print(\"Not a valid answer!\")\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            except Exception as e:\n",
    "                print(f\"Generation errors + {e}\")\n",
    "    \n",
    "    result_answer = None\n",
    "    if result_strategy == \"most_common\":\n",
    "        counter = Counter(answers)\n",
    "        result_answer = counter.most_common(1)[0][0]\n",
    "    elif result_strategy == \"random\":\n",
    "        result_answer = random.sample(answers, 1)\n",
    "    elif result_strategy == \"mean\":\n",
    "        result_answer = np.mean(answers)\n",
    "    elif result_strategy == \"first\":\n",
    "        result_answer = answers[0]\n",
    "    else:\n",
    "        result_answer = answers[-1]\n",
    "        \n",
    "    return result_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faaf289",
   "metadata": {
    "papermill": {
     "duration": 0.005748,
     "end_time": "2024-06-26T21:49:23.369532",
     "exception": false,
     "start_time": "2024-06-26T21:49:23.363784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Temperature tree generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a865025b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T21:49:23.383448Z",
     "iopub.status.busy": "2024-06-26T21:49:23.383171Z",
     "iopub.status.idle": "2024-06-26T21:49:23.416398Z",
     "shell.execute_reply": "2024-06-26T21:49:23.415450Z"
    },
    "papermill": {
     "duration": 0.043067,
     "end_time": "2024-06-26T21:49:23.418412",
     "exception": false,
     "start_time": "2024-06-26T21:49:23.375345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_start_time = None\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, tokenized_text, probability, is_leaf=False):\n",
    "        self.tokenized_text = tokenized_text\n",
    "        self.probability = probability\n",
    "        self.children = []\n",
    "        self.is_leaf = is_leaf\n",
    "        \n",
    "        \n",
    "def tree_generation(tokenized_text, temperature=1.0, max_new_tokens=96, max_tokens=2200, offset=40): # 200\n",
    "    try:\n",
    "        if tokenized_text.dim() == 1:        \n",
    "            tokenized_text = tokenized_text.unsqueeze(0)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            tokenized_text.to(model.device), \n",
    "            bad_words_ids=bad_words_ids, \n",
    "            do_sample=True, \n",
    "            top_p=5.0,\n",
    "            max_new_tokens=random.randint(max_new_tokens - offset, max_new_tokens + offset), \n",
    "            temperature=temperature,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "    \n",
    "        ret = \"continue\"\n",
    "        if len(outputs.sequences[0]) >= max_tokens:\n",
    "            ret = \"max_tokens\"\n",
    "            \n",
    "        #elif outputs.sequences[0][-1].item() == model.generation_config.eos_token_id:\n",
    "        elif model.generation_config.eos_token_id in outputs.sequences[0]:\n",
    "            ret = \"eos\"\n",
    "            \n",
    "        transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "        probability = torch.exp(transition_scores).mean().item()\n",
    "        \n",
    "        torch.cuda.empty_cache()        \n",
    "        return outputs.sequences[0], probability, ret\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Tree generation error: {e}\")\n",
    "        torch.cuda.empty_cache()   \n",
    "        return None, None, \"None\"\n",
    "        \n",
    "\n",
    "def build_tree(problem, task_prompt=\"\", temperatures=[0.80, 1.20], low_limit_probability=0.92, skip_prob=0.20, max_time=610, starting_tokens=200, start_with_generation=True): # 256\n",
    "    global global_start_time\n",
    "    global_start_time = time.time()\n",
    "    \n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": problem + \"\\n\" + task_prompt})\n",
    "    input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    \n",
    "    if start_with_generation:\n",
    "        generated_tokenized_text, probability, ret = tree_generation(input_tensor, max_new_tokens=starting_tokens, temperature=0.90)\n",
    "\n",
    "        if ret == \"continue\":\n",
    "            root = Node(generated_tokenized_text, probability)\n",
    "        else:\n",
    "            root = Node(input_tensor, 0.0)\n",
    "    else:\n",
    "        root = Node(input_tensor, 0.0)\n",
    "        \n",
    "    root = build_tree_recursive(\n",
    "        root=root, \n",
    "        temperatures=temperatures, \n",
    "        low_limit_probability=low_limit_probability, \n",
    "        skip_prob=skip_prob, \n",
    "        max_time=max_time\n",
    "    )\n",
    "    return root\n",
    "\n",
    "\n",
    "def build_tree_recursive(root, temperatures=[0.80, 1.20], low_limit_probability=0.92, skip_prob=0.20, max_time=610):\n",
    "    global global_start_time\n",
    "    elapsed_time = time.time() - global_start_time\n",
    "    \n",
    "    if elapsed_time >= max_time:\n",
    "        print(\"Time limit reached!\\n\")\n",
    "        return root\n",
    "    \n",
    "    print(f\"\\nElapsed time: {elapsed_time} seconds!\")\n",
    "    \n",
    "    if not root.is_leaf:\n",
    "        tokenized_text_root = root.tokenized_text\n",
    "        \n",
    "        random.shuffle(temperatures)\n",
    "        \n",
    "        for temperature in temperatures:\n",
    "            elapsed_time = time.time() - global_start_time\n",
    "            if elapsed_time >= max_time:\n",
    "                print(\"Time limit reached!\\n\")\n",
    "                return root\n",
    "            \n",
    "            if random.random() < skip_prob:\n",
    "                print(\"Skip!\\n\")\n",
    "                continue\n",
    "                                            \n",
    "            generated_tokenized_text, probability, ret = tree_generation(tokenized_text_root, temperature=temperature)\n",
    "            \n",
    "            print(f\"Generated tokens: {generated_tokenized_text.shape[0]}, Probability: {probability}, Return value: {ret}!\\n\")\n",
    "            \n",
    "            if ret == \"None\":\n",
    "                continue\n",
    "\n",
    "            children = Node(generated_tokenized_text, probability, ret==\"eos\")\n",
    "            \n",
    "            if ret == \"continue\" and probability >= low_limit_probability:\n",
    "                children = build_tree_recursive(\n",
    "                    root=children, \n",
    "                    temperatures=temperatures, \n",
    "                    low_limit_probability=low_limit_probability, \n",
    "                    skip_prob=skip_prob, \n",
    "                    max_time=max_time\n",
    "                )\n",
    "                    \n",
    "            if (ret == \"eos\") or (ret == \"continue\" and probability >= low_limit_probability):\n",
    "                root.children.append(children)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return root\n",
    "\n",
    "\n",
    "def find_tree_best_path_recursive(root):\n",
    "    if root.is_leaf or len(root.children) == 0:\n",
    "        return root.probability, root.tokenized_text, 1\n",
    "      \n",
    "    selected_probability = root.children[0].probability\n",
    "    selected_tokenized_text = root.children[0].tokenized_text\n",
    "    selected_n_steps = 1\n",
    "    \n",
    "    for children in root.children:\n",
    "        probability, children_tokenized_text, n_steps = find_tree_best_path_recursive(children)\n",
    "        if probability/n_steps > selected_probability/selected_n_steps:\n",
    "            selected_probability = probability\n",
    "            selected_tokenized_text = children_tokenized_text\n",
    "            selected_n_steps = n_steps\n",
    "    \n",
    "    return root.probability + selected_probability, selected_tokenized_text, selected_n_steps + 1\n",
    "\n",
    "\n",
    "def find_tree_all_paths_recursive(root, selected_paths=None):\n",
    "    if selected_paths is None:\n",
    "        selected_paths = []\n",
    "    \n",
    "    if root.is_leaf:        \n",
    "        selected_paths.append(root)\n",
    "\n",
    "    if len(root.children) > 0:        \n",
    "        for children in root.children:\n",
    "            find_tree_all_paths_recursive(children, selected_paths)\n",
    "        \n",
    "    return selected_paths\n",
    "    \n",
    "    \n",
    "def answer_generation_tree(\n",
    "    problem, \n",
    "    task_prompt,\n",
    "    max_iterations=2, # 3\n",
    "    temperatures=[0.75, 0.825, 0.90, 1.0], #[0.80, 1.25]\n",
    "    low_limit_probability=0.88, # 0.9215\n",
    "    skip_prob=0.05, #0.215\n",
    "    max_time=610, # 630\n",
    "    start_with_generation=False,\n",
    "    result_strategy=\"most_common\"\n",
    "):\n",
    "    \n",
    "    iteration = 1\n",
    "    while(True):\n",
    "        print(f\"\\n----- Iteration {iteration} -----\")\n",
    "        try:\n",
    "            root = build_tree(\n",
    "                problem=problem, \n",
    "                task_prompt=task_prompt, \n",
    "                temperatures=temperatures, \n",
    "                low_limit_probability=low_limit_probability, \n",
    "                skip_prob=skip_prob,\n",
    "                max_time=max_time, \n",
    "                start_with_generation=start_with_generation\n",
    "            )\n",
    "            \n",
    "            if result_strategy == \"best\":\n",
    "                best_probability, best_tokenized_text, _ = find_tree_best_path_recursive(root)\n",
    "                output = tokenizer.decode(best_tokenized_text, skip_special_tokens=True)\n",
    "                answer, correct = process_output(output)\n",
    "                break\n",
    "                \n",
    "            elif result_strategy == \"most_common\":\n",
    "                selected_nodes = find_tree_all_paths_recursive(root)\n",
    "                answers = []\n",
    "                for node in selected_nodes:\n",
    "                    output = tokenizer.decode(node.tokenized_text, skip_special_tokens=True)\n",
    "                    answer, correct = process_output(output)\n",
    "                    if correct:\n",
    "                        answers.append(answer)\n",
    "                \n",
    "                counter = Counter(answers)\n",
    "                answer = counter.most_common(1)[0][0]\n",
    "                break\n",
    "                    \n",
    "            elif result_strategy == \"best_valid\":\n",
    "                selected_nodes = find_tree_all_paths_recursive(root)\n",
    "                selected_nodes.sort(key=lambda node: node.probability, reverse=True)\n",
    "                for node in selected_nodes:\n",
    "                    output = tokenizer.decode(node.tokenized_text, skip_special_tokens=True)\n",
    "                    answer, correct = process_output(output)\n",
    "                    if correct:\n",
    "                        break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Answer generation error for iteration {iteration}: {e}\")\n",
    "            answer = random.randint(0, 999)\n",
    "                \n",
    "        if iteration == max_iterations:\n",
    "            break\n",
    "        \n",
    "        iteration += 1\n",
    "        low_limit_probability -= 0.075\n",
    "        max_time -= 250\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c663399",
   "metadata": {
    "papermill": {
     "duration": 0.00588,
     "end_time": "2024-06-26T21:49:23.430724",
     "exception": false,
     "start_time": "2024-06-26T21:49:23.424844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd3a0604",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T21:49:23.444082Z",
     "iopub.status.busy": "2024-06-26T21:49:23.443761Z",
     "iopub.status.idle": "2024-06-26T21:49:23.453532Z",
     "shell.execute_reply": "2024-06-26T21:49:23.452490Z"
    },
    "papermill": {
     "duration": 0.018659,
     "end_time": "2024-06-26T21:49:23.455402",
     "exception": false,
     "start_time": "2024-06-26T21:49:23.436743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AVAILABLE MODES: tot, single, few, temperature_tree\n",
    "# n_shots: used only in if mode == few\n",
    "# n_iteration: to run the generation many times\n",
    "mode = \"temperature_tree\"  \n",
    "n_iteration = 1\n",
    "n_shots = 2    # \n",
    "result_strategy = \"most_common\" \n",
    "\n",
    "\n",
    "examples = {\n",
    "    \"algebra\" : {\n",
    "        \"problem\"  : \"Compute $\\\\frac{x^8+12x^4+36}{x^4+6}$ when $x=5$.\",\n",
    "        \"solution\" : \"Note that $\\\\left(x^4+6\\\\right)^2=x^8+12x^4+36$. So $\\\\frac{x^8+12x^4+36}{x^4+6}=\\\\frac{\\\\left(x^4+6\\\\right)^2}{x^4+6}=x^4+6$. Our answer is therefore $5^4+6=625+6=\\\\boxed{631}$.\"\n",
    "    },\n",
    "    \n",
    "    \"Geometry\" : {\n",
    "        \"problem\"  : \"What is the smallest possible perimeter, in units, of a triangle whose side-length measures are consecutive integer values?\",\n",
    "        \"solution\" : \"The smallest such triangle has lengths 1, 2, and 3. However, this triangle doesn't work since the sum of any two side lengths must be greater than the third side length (by the Triangle Inequality). The next smallest triangle has lengths 2, 3, and 4, which works. Thus, the smallest possible perimeter is $2+3+4=\\\\boxed{9}$ units.\"\n",
    "    },\n",
    "    \n",
    "    \"Intermediate Algebra\" : {\n",
    "        \"problem\"  : \"Given that $a-b=5$ and $a^2+b^2=35$, find $a^3-b^3$.\",\n",
    "        \"solution\" : \"We know that $(a-b)^2=a^2-2ab+b^2$. Therefore, we plug in the given values to get $5^2=35-2ab$. Solving, we get that $ab=5$. We also have the difference of cubes factorization $a^3-b^3=(a-b)(a^2+ab+b^2)$. Plugging in the values given and solving, we get that $a^3-b^3=(5)(35+5)=(5)(40)=\\\\boxed{200}$.\"\n",
    "    },\n",
    "    \n",
    "    \"Prealgebra\" : {\n",
    "        \"problem\"  : \"What is the greatest integer $x$ for which $\\\\frac79 > \\\\frac{x}{13}$?\",\n",
    "        \"solution\" : \"Multiplying both sides of the inequality by $13$, we have $\\\\frac{91}{9}>x$. The largest integer smaller than $\\\\frac{91}{9}$ is $\\\\boxed{10}$.\"\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "task_prompts = [\n",
    "    \"Please reason step by step, and put your final answer within \\\\boxed{}.\",\n",
    "    \"Given the problem, reason step by step, and compute the answer. The final answer is not an expression but is a positive integer and must be inserted within \\\\boxed{}.\",\n",
    "    \"To solve the problem, show all the steps. Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression! Then output the final integer answer within \\\\boxed{}.\",\n",
    "    \"Please show all the steps to reach the final answer which is a positive integer and put it within \\\\boxed{}.\",\n",
    "    \"Solve the puzzle step by step. Remember, a step is only valid if it is a piece that leads to the final correct answer (positive integer). Don't add it otherwise!. Enter your final answer within \\\\boxed{}.\",\n",
    "    \"There is only one valid answer. This is a positive integer and not an expression or a literal and must be inserted within \\\\boxed{}. List the steps to reach it.\",\n",
    "    \"Provide the final answer within \\\\boxed{}, justifying your answer in detail. The final answer is a positive integer and not an expression or a literal.\",\n",
    "    \"Thinking step by step, find an answer and put it within \\\\boxed{}.\"\n",
    "]\n",
    "\n",
    "\n",
    "tot_prompts = [\n",
    "    \"\"\"Imagine three different experts who think step by step to solve the problem.\n",
    "    Each of the experts propose a step of its reasoning, then share it with the group.\n",
    "    Experts agree on the correctness and then will go on to the next step.\n",
    "    Experts put the final answer within \\\\boxed{}.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"Imagine three different experts who think step by step to solve the problem.\n",
    "    The first expert proposes the setps, while the other two in sequence check the correctness of each step, correcting them if necessary.\n",
    "    Experts put the final answer within \\\\boxed{}.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "tree_task_prompt = \"Please reason step by step, and put your final answer within \\\\boxed{}.\"\n",
    "\n",
    "if mode == \"tot\":\n",
    "    prompts = tot_prompts\n",
    "else:\n",
    "    prompts = task_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3ff21",
   "metadata": {
    "papermill": {
     "duration": 0.005462,
     "end_time": "2024-06-26T21:49:23.466809",
     "exception": false,
     "start_time": "2024-06-26T21:49:23.461347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ab589eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T21:49:23.479864Z",
     "iopub.status.busy": "2024-06-26T21:49:23.479609Z",
     "iopub.status.idle": "2024-06-26T21:49:23.486480Z",
     "shell.execute_reply": "2024-06-26T21:49:23.485737Z"
    },
    "papermill": {
     "duration": 0.015642,
     "end_time": "2024-06-26T21:49:23.488340",
     "exception": false,
     "start_time": "2024-06-26T21:49:23.472698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST:\n",
    "    df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        print(f\"\\n\\n######################## Question {i + 1} ########################\")\n",
    "        try:\n",
    "            problem = df['problem'].loc[i]\n",
    "            real_answer = df['answer'].loc[i]\n",
    "            \n",
    "            if mode != \"temperature_tree\":\n",
    "                predicted_answer = answer_generation(problem, \n",
    "                                                     examples, \n",
    "                                                     task_prompts=prompts, \n",
    "                                                     mode=mode, \n",
    "                                                     n_shots=n_shots, \n",
    "                                                     n_iteration=n_iteration,\n",
    "                                                     result_strategy=result_strategy)\n",
    "            \n",
    "            else:\n",
    "                predicted_answer = answer_generation_tree(problem=problem, task_prompt=tree_task_prompt)\n",
    "                \n",
    "            print(f\"\\nPrecited answer: {predicted_answer}\")\n",
    "            print(f\"\\nReal answer: {real_answer}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Some generation errors: {e}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e93093",
   "metadata": {
    "papermill": {
     "duration": 0.005689,
     "end_time": "2024-06-26T21:49:23.499706",
     "exception": false,
     "start_time": "2024-06-26T21:49:23.494017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "517efa2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T21:49:23.512267Z",
     "iopub.status.busy": "2024-06-26T21:49:23.512012Z",
     "iopub.status.idle": "2024-06-26T21:50:43.050761Z",
     "shell.execute_reply": "2024-06-26T21:50:43.049740Z"
    },
    "papermill": {
     "duration": 79.547425,
     "end_time": "2024-06-26T21:50:43.052902",
     "exception": false,
     "start_time": "2024-06-26T21:49:23.505477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "\n",
      "\n",
      "######################## New Question ########################\n",
      "\n",
      "----- Iteration 1 -----\n",
      "\n",
      "Elapsed time: 0.053858280181884766 seconds!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 21:49:26.878382: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-26 21:49:26.878494: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-26 21:49:27.042741: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens: 77, Probability: 0.818625807762146, Return value: eos!\n",
      "\n",
      "Generated tokens: 85, Probability: 0.8369529247283936, Return value: eos!\n",
      "\n",
      "Generated tokens: 75, Probability: 0.8716273903846741, Return value: eos!\n",
      "\n",
      "Generated tokens: 67, Probability: 0.8256813883781433, Return value: eos!\n",
      "\n",
      "       id         problem\n",
      "0  000aaa  What is $1-1$?\n",
      "0\n",
      "\n",
      "\n",
      "######################## New Question ########################\n",
      "\n",
      "----- Iteration 1 -----\n",
      "\n",
      "Elapsed time: 0.0006647109985351562 seconds!\n",
      "Generated tokens: 67, Probability: 0.8676378726959229, Return value: eos!\n",
      "\n",
      "Generated tokens: 64, Probability: 0.868796706199646, Return value: eos!\n",
      "\n",
      "Generated tokens: 64, Probability: 0.9029936790466309, Return value: eos!\n",
      "\n",
      "Generated tokens: 66, Probability: 0.8403667211532593, Return value: eos!\n",
      "\n",
      "       id               problem\n",
      "0  111bbb  What is $0\\times10$?\n",
      "0\n",
      "\n",
      "\n",
      "######################## New Question ########################\n",
      "\n",
      "----- Iteration 1 -----\n",
      "\n",
      "Elapsed time: 0.0007126331329345703 seconds!\n",
      "Generated tokens: 142, Probability: 0.902799665927887, Return value: continue!\n",
      "\n",
      "\n",
      "Elapsed time: 6.482324838638306 seconds!\n",
      "Generated tokens: 184, Probability: 0.921640157699585, Return value: eos!\n",
      "\n",
      "Generated tokens: 183, Probability: 0.9447756409645081, Return value: eos!\n",
      "\n",
      "Generated tokens: 181, Probability: 0.940489649772644, Return value: eos!\n",
      "\n",
      "Generated tokens: 168, Probability: 0.8437367677688599, Return value: eos!\n",
      "\n",
      "Generated tokens: 108, Probability: 0.923150360584259, Return value: continue!\n",
      "\n",
      "\n",
      "Elapsed time: 20.451321125030518 seconds!\n",
      "Generated tokens: 158, Probability: 0.9478152990341187, Return value: eos!\n",
      "\n",
      "Generated tokens: 156, Probability: 0.9461829662322998, Return value: eos!\n",
      "\n",
      "Generated tokens: 157, Probability: 0.9345843195915222, Return value: eos!\n",
      "\n",
      "Generated tokens: 159, Probability: 0.9331205487251282, Return value: eos!\n",
      "\n",
      "Generated tokens: 134, Probability: 0.8978390693664551, Return value: continue!\n",
      "\n",
      "\n",
      "Elapsed time: 39.07490611076355 seconds!\n",
      "Generated tokens: 162, Probability: 0.9244492053985596, Return value: eos!\n",
      "\n",
      "Generated tokens: 164, Probability: 0.8928337693214417, Return value: eos!\n",
      "\n",
      "Generated tokens: 161, Probability: 0.9666181802749634, Return value: eos!\n",
      "\n",
      "Generated tokens: 161, Probability: 0.9310734868049622, Return value: eos!\n",
      "\n",
      "Skip!\n",
      "\n",
      "       id                 problem\n",
      "0  222ccc  Solve $4+x=4$ for $x$.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "if not IS_TEST:\n",
    "    import aimo\n",
    "\n",
    "    env = aimo.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    \n",
    "    for test, sample_submission in iter_test:\n",
    "        print(f\"\\n\\n######################## New Question ########################\")\n",
    "        try:\n",
    "            problem = test['problem'].values[0]\n",
    "            \n",
    "            if mode != \"temperature_tree\":\n",
    "                predicted_answer = answer_generation(problem, \n",
    "                                                     examples, \n",
    "                                                     task_prompts=prompts, \n",
    "                                                     mode=mode, \n",
    "                                                     n_shots=n_shots, \n",
    "                                                     n_iteration=n_iteration,\n",
    "                                                     result_strategy=result_strategy)\n",
    "            else:\n",
    "                predicted_answer = answer_generation_tree(problem=problem, task_prompt=tree_task_prompt)\n",
    "        except Exception as e:\n",
    "            print(f\"Some generation errors: {e}\\n\\n\")\n",
    "            predicted_answer = random.randint(0, 999)\n",
    "        \n",
    "        print(test)\n",
    "        print(predicted_answer)\n",
    "        \n",
    "        sample_submission['answer'] = predicted_answer\n",
    "        env.predict(sample_submission)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8365361,
     "sourceId": 73231,
     "sourceType": "competition"
    },
    {
     "datasetId": 4717118,
     "sourceId": 8008788,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4728129,
     "sourceId": 8023365,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 164836055,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 227.477661,
   "end_time": "2024-06-26T21:50:46.514542",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-26T21:46:59.036881",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "203bafce45bf47d4966c426351c73c8c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "23ecf0fb61fd48f28fd39e74dfd638cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9bc89053a2664eb2bb2a2f5f461caf24",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c200e95a65b0438c9ced8c1f8322f8d5",
       "value": 3.0
      }
     },
     "524fce5682594320b9867d1d325a345e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "581c1b0b3cb5451b9b15e35766f7fa49": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9e635cc9955c4b1f8c9138165a885670",
        "IPY_MODEL_23ecf0fb61fd48f28fd39e74dfd638cf",
        "IPY_MODEL_c124136abd304a19bfaace867f70dc38"
       ],
       "layout": "IPY_MODEL_203bafce45bf47d4966c426351c73c8c"
      }
     },
     "7051b54f8e454cd6ad154336910b7c85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9bc89053a2664eb2bb2a2f5f461caf24": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9e635cc9955c4b1f8c9138165a885670": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_524fce5682594320b9867d1d325a345e",
       "placeholder": "",
       "style": "IPY_MODEL_ef94ba4501104bb4b64a990b2e581b8b",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "c02b167f7fbf4dd48207f37e53689b54": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c124136abd304a19bfaace867f70dc38": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7051b54f8e454cd6ad154336910b7c85",
       "placeholder": "",
       "style": "IPY_MODEL_c02b167f7fbf4dd48207f37e53689b54",
       "value": " 3/3 [01:39&lt;00:00, 31.22s/it]"
      }
     },
     "c200e95a65b0438c9ced8c1f8322f8d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ef94ba4501104bb4b64a990b2e581b8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
